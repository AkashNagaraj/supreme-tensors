{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1115393 data-characters and 65 unique characters\n",
      "---------sample----------\n",
      "---------\n",
      " $vmX\n",
      "bOJM;PGAQF&PyPPS$RF,Jp$FnztfDDHZlUyF FxyGRK,SSLGRGqeE'b'xTpgVLj3ybmTRfVETfEn'ImuTtQEGOR$xfrF'kqTMMa&TxdELDMLjxRISnAKRgap\n",
      "NTZE?lPNrdxwLyQiXGL SrR:VxordvHEcf.:Xm?xlNXqHcdWJpsJVxo'cqep3mZdlP'I3xxFlz \n",
      "--------\n",
      "Iter - 0, Loss - 104.8150415510979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:151: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f7abdc7c4e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# prepare inputs (left -> right in steps of seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m#reset RNN memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mhprev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=open('data/input.txt').read()\n",
    "chars=list(set(data))\n",
    "data_size,vocab_size=len(data),len(chars)\n",
    "print('Data has {} data-characters and {} unique characters'.format(data_size,vocab_size))\n",
    "\n",
    "char_to_idx={ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char={i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "#hyperparameters\n",
    "hidden_size=100 # size of hidden layer of neurons\n",
    "seq_length=25 # number of steps to unroll the RNN\n",
    "learning_rate=1e-1\n",
    "\n",
    "# update the hidden state\n",
    "# self.h=np.tanh(np.dot(self.W_hh,self.h)+np.dot(self.W_xh,x))\n",
    "# compute the new output vector\n",
    "# y=np.dot(self.W_hy,self.h)\n",
    "\n",
    "Wxh=np.random.randn(hidden_size,vocab_size) #input to hidden\n",
    "Whh=np.random.randn(hidden_size,hidden_size) #hidden to hidden\n",
    "Why=np.random.randn(vocab_size,hidden_size) #hidden to output\n",
    "bh=np.zeros((hidden_size,1)) #hidden bias\n",
    "by=np.zeros((vocab_size,1)) #output bias\n",
    "\n",
    "#compute loss, derivative\n",
    "#cross-entropy error and sum of squared errors in backpropagation[since output is linear]\n",
    "\n",
    "def lossFunc(inputs,targets,hprev):\n",
    "    xs,hs,ys,ps={},{},{},{}\n",
    "    \n",
    "    hs[-1]=np.copy(hprev)\n",
    "    loss=0\n",
    "    \n",
    "    # forward pass for each training data point\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t]=np.zeros((vocab_size,1))\n",
    "        xs[t][inputs[t]]=1\n",
    "        \n",
    "        #hidden state using previous hidden state hs[t-1]\n",
    "        hs[t]=np.tanh(np.dot(Wxh,xs[t])+np.dot(Whh,hs[t-1])+bh)\n",
    "        #unnormalized log probabilities for next chars\n",
    "        ys[t]=np.dot(Why,hs[t])+by\n",
    "        #probabilities for next chars[softmax]\n",
    "        ps[t]=np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        #cross entropy loss        \n",
    "        loss+=-np.log(ps[t][targets[t],0]) \n",
    "        \n",
    "    # backward pass : compute gradients going backwards\n",
    "    dWxh,dWhh,dWhy=np.zeros_like(Wxh),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "    dbh,dby=np.zeros_like(bh),np.zeros_like(by)\n",
    "    \n",
    "    dhnext=np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #compute derivative of error w.r.t the output probs\n",
    "        # dE/dy[j]=y[j]-t[j]\n",
    "        dy=np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 \n",
    "        \n",
    "        #output does not use activation function\n",
    "        #therefore derivative of error with regard to the weight between hidden and output layer:\n",
    "        #dE/dy[j]*dy[j]/dWhy[j,k]=dE/dy[j]*h[k]\n",
    "        dWhy+=np.dot(dy,hs[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        #backprop into h\n",
    "        #derivative of H comes from output layer y and from next hidden layer H[t+1] \n",
    "        dh=np.dot(dWhy.T,dy)+dhnext\n",
    "        \n",
    "        #backprop through tanh nonlinearity\n",
    "        #dtanhx/dx=1-tanh(x)*tanh(x)\n",
    "        dhraw=(1-hs[t]*hs[t])*dh\n",
    "        dbh+=dhraw\n",
    "        \n",
    "        #derivative of error with regard to the weight between input and hidden layer\n",
    "        dWxh += np.dot(dhraw,xs[t].T)\n",
    "        dWhh += np.dot(dhraw,hs[t-1].T)\n",
    "        \n",
    "        #derivative of error with regard to H(t+1)\n",
    "        dhnext=np.dot(Whh.T,dhraw)\n",
    "        \n",
    "    for dparam in [dWxh,dWhh,dWhy,dbh,dby]:\n",
    "        np.clip(dparam,5,-5,out=dparam)\n",
    "        \n",
    "    return loss,dWxh,dWhh,dWhy,dbh,dby,hs[len(inputs)-1]\n",
    "\n",
    "def sample(h,seed_ix,n):\n",
    "\n",
    "    # sample a seq of integers from the model\n",
    "    # h -> memory state, seed_ix -> seed letter for 1st time step\n",
    "    \n",
    "    #one-hot\n",
    "    x=np.zeros((vocab_size,1))\n",
    "    x[seed_ix]=1\n",
    "    \n",
    "    ixes=[]\n",
    "    for t in range(n):\n",
    "        h=np.tanh(np.dot(Wxh,x)+np.dot(Whh,h)+bh)\n",
    "        y=np.dot(Why,h)+by\n",
    "        p=np.exp(y)/np.sum(np.exp(y))\n",
    "        #sample according to probability distribution\n",
    "        ix=np.random.choice(range(vocab_size),p=p.ravel())\n",
    "        \n",
    "        #update input x\n",
    "        x=np.zeros((vocab_size,1))\n",
    "        x[ix]=1\n",
    "        \n",
    "        ixes.append(ix)\n",
    "        \n",
    "    return ixes\n",
    "\n",
    "#iterator\n",
    "n=0\n",
    "#data pointer\n",
    "p=0\n",
    "\n",
    "mWxh,mWhh,mWhy=np.zeros_like(Wxh),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "mbh,mby=np.zeros_like(bh),np.zeros_like(by) # meemory variables for adagrad\n",
    "smooth_loss=-np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "while True:\n",
    "    # prepare inputs (left -> right in steps of seq_length)\n",
    "    if p+seq_length+1>=len(data) or n==0:\n",
    "        #reset RNN memory\n",
    "        hprev=np.zeros((hidden_size,1)) # hidden state\n",
    "        p=0\n",
    "        \n",
    "        inputs=[char_to_idx[ch] for ch in data[p:p+seq_length]]\n",
    "        targets=[char_to_idx[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "        \n",
    "        #sample from the model\n",
    "        if n%100==0:\n",
    "            sample_ix=sample(hprev,inputs[0],200)\n",
    "            txt=''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print('---------sample----------')\n",
    "            print('---------\\n {} \\n--------'.format(txt))\n",
    "            \n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFunc(inputs,targets,hprev)\n",
    "        \n",
    "        #autograd\n",
    "        smooth_loss=smooth_loss*0.999 +loss*0.001\n",
    "        if n%2==0:\n",
    "            print('Iter - {}, Loss - {}'.format(n,smooth_loss))\n",
    "            \n",
    "        #memory is accumulated after each iteration\n",
    "        for param, dparam, mem in zip([Wxh,Whh,Why,bh,by],[dWxh,dWhh,dWhy,dbh,dby],[mWxh,mWhh,mWhy,mbh,mby]):\n",
    "            mem+=param*dparam\n",
    "            # learning rate adjusted by mem\n",
    "            param+=-learning_rate*dparam/np.sqrt(mem+1e-8)\n",
    "            \n",
    "        p+=seq_length\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
