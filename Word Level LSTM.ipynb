{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, Activation\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    directory='data/input.txt'\n",
    "    file=open(directory).read().lower().replace('\\n',' \\n ')\n",
    "    print('Corpus length in characters : {}'.format(len(file)))\n",
    "    text_in_words=[w for w in file.split(' ') if w.strip() !='' or w=='\\n']\n",
    "    print('Corpus length in words : {}'.format(len(text_in_words)))\n",
    "\n",
    "    # Calculate word frequencies\n",
    "\n",
    "    #hyperparameter\n",
    "    MIN_WORD_FREQUENCY=3\n",
    "\n",
    "    word_freq={}\n",
    "    for w in text_in_words:\n",
    "        word_freq[w]=word_freq.get(w,0)+1\n",
    "    \n",
    "    ignored_words=set()\n",
    "    for k,v in word_freq.items():\n",
    "        if(word_freq[k]<MIN_WORD_FREQUENCY):\n",
    "            ignored_words.add(k)\n",
    "        \n",
    "    words=set(text_in_words)\n",
    "    print(\"Unique words before ignoring : {}\".format(len(words)))\n",
    "    print(\"Ignoring words with frequency < : {}\".format(MIN_WORD_FREQUENCY))\n",
    "    words=sorted(words-ignored_words)\n",
    "    print(\"Unique words after ignoring : {}\".format(len(words)))\n",
    "\n",
    "    word_indices={word:i for i,word in enumerate(words)}\n",
    "    indices_word={i:word for i,word in enumerate(words)}\n",
    "    \n",
    "    return word_indices, indices_word, text_in_words, ignored_words, MIN_WORD_FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and Filtering the Sequence\n",
    "def filter_and_sequence(text_in_words, ignored_words):\n",
    "    \n",
    "    #hyperparameters\n",
    "    SEQUENCE_LEN=8\n",
    "    \n",
    "    STEP=1\n",
    "    sentences=[]\n",
    "    next_words=[]\n",
    "    ignored=0\n",
    "    for i in range(0,len(text_in_words)-SEQUENCE_LEN,STEP):\n",
    "        if len(set(text_in_words[i:i+SEQUENCE_LEN+1]).intersection(ignored_words))==0:\n",
    "            sentences.append(text_in_words[i:i+SEQUENCE_LEN])\n",
    "            next_words.append(text_in_words[i+SEQUENCE_LEN])\n",
    "        else:\n",
    "            ignored=ignored+1\n",
    "    \n",
    "    print('Ignored Sentences : {}'.format(ignored))\n",
    "    print('Remaining Sequences : {}'.format(len(sentences)))\n",
    "    \n",
    "    return sentences, next_words, SEQUENCE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Set shuffling\n",
    "def shuffle_and_split(sentences,next_words):\n",
    "    \n",
    "    combined=list(zip(sentences,next_words))\n",
    "    random.shuffle(combined)\n",
    "    sentences[:],next_words[:]=zip(*combined)\n",
    "    \n",
    "    #train test split ratio is 90-10\n",
    "    sentences_test=sentences[int(len(sentences)*0.9):]\n",
    "    next_words_test=next_words[int(len(next_words)*0.9):]\n",
    "    sentences=sentences[:int(len(sentences)*0.9)]\n",
    "    next_words=next_words[:int(len(next_words)*0.9)]\n",
    "    \n",
    "    return sentences, next_words, sentences_test, next_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "def get_model(dropout=0.2):\n",
    "    model=Sequential()\n",
    "    model.add(Bidirectional(LSTM(128),input_shape=(SEQUENCE_LEN,len(words))))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "def data_generator(sentence_list,next_word_list,batch_size):\n",
    "    index=0\n",
    "    while True:\n",
    "        x=np.zeros((batch_size,SEQUENCE_LEN,len(words)),dtype=np.bool)\n",
    "        y=np.zeros((batch_size,len(words)),dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t,w in enumerate(sentence_list[index%len(sentence_list)]):\n",
    "                x[i,t,word_indices[w]]=1\n",
    "            y[i,word_indices[next_word_list[index%len(sentence_list)]]]=1\n",
    "            index=index+1\n",
    "    yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end():\n",
    "    data_generator()\n",
    "    for i in text_in_words:\n",
    "        model.preds(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters : 1195391\n",
      "Corpus length in words : 242650\n",
      "Unique words before ignoring : 23642\n",
      "Ignoring words with frequency < : 3\n",
      "Unique words after ignoring : 6642\n",
      "Ignored Sentences : 130548\n",
      "Remaining Sequences : 112094\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'on_epoch_end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-ad81983f2c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambdaCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'on_epoch_end' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    if not os.path.isdir('./checkpoints/'):\n",
    "        os.makedirs('./checkpoints/')\n",
    "    \n",
    "    word_indices, indices_word, text_in_words, ignored_words, MIN_WORD_FREQUENCY = preprocessing()\n",
    "    sentences, next_words, SEQUENCE_LEN= filter_and_sequence(text_in_words, ignored_words)\n",
    "    sentences, next_words, sentences_test, next_words_test = shuffle_and_split(sentences, next_words)\n",
    "    \n",
    "    BATCH_SIZE=1100 # find ideal value\n",
    "    model = get_model()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    file_path=\"./checkpoints/LSTM-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\"\\\n",
    "              \"loss{}-acc{}-val_loss{}-val_acc{}\"%\\\n",
    "                (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(file_path,monitor='val_acc',save_best_only=True)\n",
    "    print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "    early_stopping=EarlyStopping(monitor='val_acc',patience=5)\n",
    "    callbacks_list=[checkpoint,print_callback,early_stopping]\n",
    "    \n",
    "    model.fit_generator(data_generator(sentences,next_words,BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE+1),\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(sentences_test,next_words_test,BATCH_SIZE),\n",
    "                        validation_steps=int(len(sentences_test)/BATCH_SIZE+1)\n",
    "                       )\n",
    "    \n",
    "    sentences, next_words, sentences_test, next_words_test=shuffle_and_split(sentences,next_words)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
